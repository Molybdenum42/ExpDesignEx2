{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-8cf31059-e827-4e3f-ae69-7428321b8532",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c0f8731d",
    "execution_start": 1641236436145,
    "execution_millis": 16,
    "deepnote_cell_type": "code"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from urllib.parse import urlparse\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-28debeff-6e57-45b0-9642-60152375fdaa",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "19a35659",
    "execution_start": 1641236448609,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "source": [
    "import string\n",
    "\n",
    "def load_data():\n",
    "    filename = \"../../data/twitter_data_fixed.pkl\"\n",
    "    print(\"Loading data from file: \" + filename)\n",
    "    data = pickle.load(open(filename, 'rb'))\n",
    "    x_text = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "    for i in range(len(data)):\n",
    "        text = \"\".join(l for l in data[i]['text'] if l not in string.punctuation)\n",
    "        x_text.append((data[i]['text']).encode('utf-8'))\n",
    "        labels.append(data[i]['label'])\n",
    "    return x_text,labels"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-50e0c8b7-4bbf-4d9b-9839-4dbeadec16da",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3ebe17b2",
    "execution_start": 1641237141898,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": [
    "def is_url(url):\n",
    "  try:\n",
    "    result = urlparse(url)\n",
    "    return all([result.scheme, result.netloc])\n",
    "  except ValueError:\n",
    "    return False"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-384604ca-5f01-4fb1-a6de-8cf027ccfe59",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b8e7fefa",
    "execution_start": 1641236737861,
    "execution_millis": 663,
    "deepnote_cell_type": "code"
   },
   "source": [
    "x_text, labels_og = load_data()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: ../../data/twitter_data_fixed.pkl\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "labels, uniques = pd.factorize(labels_og)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00005-7a994976-4250-473c-9365-7c598a214c5e",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3328cee7",
    "execution_start": 1641236894129,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": [
    "comments = pd.DataFrame({'comment': x_text, 'attack': labels})\n",
    "\n",
    "# decode to UTF-8\n",
    "comments['comment'] = comments['comment'].str.decode(\"utf-8\")\n",
    "\n",
    "# remove missing rows\n",
    "comments['comment'].dropna(inplace=True)\n",
    "\n",
    "# remove usernames\n",
    "comments['comment'] = comments['comment'].str.replace('(\\@\\w+.*?)',\"\", regex=True)\n",
    "\n",
    "# lower case everything\n",
    "comments['comment'] = comments['comment'].str.lower()\n",
    "\n",
    "# remove URLs\n",
    "comments['comment'] = [' '.join(y for y in x.split() if not is_url(y)) for x in comments['comment']]\n",
    "\n",
    "# remove stop words\n",
    "comments['comment'] = comments['comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))\n",
    "\n",
    "# tokenize\n",
    "tt = TweetTokenizer()\n",
    "comments['comment'] = [tt.tokenize(entry) for entry in comments['comment']]\n",
    "\n",
    "# remove punctuation\n",
    "comments['comment'] = [list(filter(lambda x: x not in string.punctuation, sentence)) for sentence in comments['comment']]\n",
    "\n",
    "# traditionally, would also lemmatize but this was not done in the main data\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BENLEE~1\\AppData\\Local\\Temp/ipykernel_7728/2417445224.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  comments['comment'] = comments['comment'].str.replace('(\\@\\w+.*?)',\"\")\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 comment  attack\n",
      "0      [rt, another, bloody, instant, restaurant, wee...       0\n",
      "1      [video, peshmerga, decimating, isis, far, inte...       0\n",
      "2      [oh, really, instant, restaurants, that's, sho...       0\n",
      "3      [rt, good, weeks, #isis, new, front, opened, #...       0\n",
      "4      [rt, don, â€™, t, need, femisnsn, men, carry, he...       0\n",
      "...                                                  ...     ...\n",
      "16085  [rt, i, want, equal, rights, still, want, seat...       2\n",
      "16086  [rt, go, ahead, call, sexist, scandalous, wome...       2\n",
      "16087  [epic, always, kept, plugged, in, plugged, use...       0\n",
      "16088  [think, daesh, planning, second, battle, trenc...       0\n",
      "16089  [rt, skin, green, colors, suit, wear, ripped, ...       0\n",
      "\n",
      "[16090 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(comments)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# initial preprocessed file\n",
    "comments.to_csv(\"../../data/twitter_data_DLpreprocessed.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# triple oversample in original paper is similar to equalizing numbers\n",
    "\n",
    "def oversample(df):\n",
    "    classes = df.attack.value_counts().to_dict()\n",
    "    most = max(classes.values())\n",
    "    classes_list = []\n",
    "    for key in classes:\n",
    "        classes_list.append(df[df['attack'] == key])\n",
    "    classes_sample = []\n",
    "    for i in range(1,len(classes_list)):\n",
    "        classes_sample.append(classes_list[i].sample(most, replace=True))\n",
    "    df_maybe = pd.concat(classes_sample)\n",
    "    final_df = pd.concat([df_maybe,classes_list[0]], axis=0)\n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "    return final_df\n",
    "\n",
    "comments_oversampled  = oversample(comments)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 comment  attack\n",
      "0      [rt, happens, men, time, men, reproductive, ri...       2\n",
      "1      [rt, i'm, sexist, damn, let, females, act, lik...       2\n",
      "2      [rt, hate, u, open, door, hot, chick, ugly, fr...       2\n",
      "3      [tl, wanted, say, ty, great, work, challenging...       2\n",
      "4                                       [also, existing]       2\n",
      "...                                                  ...     ...\n",
      "33103  [great, see, colin, got, bed, get, together, #...       0\n",
      "33104                            [live, know, ca, drive]       0\n",
      "33105  [epic, always, kept, plugged, in, plugged, use...       0\n",
      "33106  [think, daesh, planning, second, battle, trenc...       0\n",
      "33107  [rt, skin, green, colors, suit, wear, ripped, ...       0\n",
      "\n",
      "[33108 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(comments_oversampled)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "# oversampled preprocessed file\n",
    "comments_oversampled.to_csv(\"../../data/twitter_data_DLpreprocessed_oversampled.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "1f258d41-bc49-4b9b-9d50-a7a491b90b36",
  "deepnote_execution_queue": []
 }
}