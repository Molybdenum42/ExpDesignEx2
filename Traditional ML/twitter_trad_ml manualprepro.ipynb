{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-8cf31059-e827-4e3f-ae69-7428321b8532",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c0f8731d",
    "execution_start": 1641236436145,
    "execution_millis": 16,
    "deepnote_cell_type": "code"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from urllib.parse import urlparse\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-28debeff-6e57-45b0-9642-60152375fdaa",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "19a35659",
    "execution_start": 1641236448609,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "source": [
    "import string\n",
    "\n",
    "def load_data():\n",
    "    filename = \"../data/twitter_data_fixed.pkl\"\n",
    "    print(\"Loading data from file: \" + filename)\n",
    "    data = pickle.load(open(filename, 'rb'))\n",
    "    x_text = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "    for i in range(len(data)):\n",
    "        text = \"\".join(l for l in data[i]['text'] if l not in string.punctuation)\n",
    "        x_text.append((data[i]['text']).encode('utf-8'))\n",
    "        labels.append(data[i]['label'])\n",
    "    return x_text,labels"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-50e0c8b7-4bbf-4d9b-9839-4dbeadec16da",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3ebe17b2",
    "execution_start": 1641237141898,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": [
    "def get_model(name):\n",
    "    if name == \"lr\":\n",
    "        model = LogisticRegression()\n",
    "    elif name == \"svm\":\n",
    "        model = LinearSVC()\n",
    "    elif name == \"rf\":\n",
    "        model = RandomForestClassifier(n_jobs=-1)\n",
    "    elif name == \"nb\":\n",
    "        model = MultinomialNB()\n",
    "    else:\n",
    "        return None\n",
    "    return model\n",
    "\n",
    "def evaluate_model(modelname, features, labels):\n",
    "    model = get_model(modelname)\n",
    "    kfoldcv = KFold(n_splits = n_folds)\n",
    "    scores = []\n",
    "    n = 0\n",
    "    for train_index, test_index in kfoldcv.split(features):\n",
    "        n += 1\n",
    "        print(n)\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        model.fit(X=X_train, y=y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred, average=None)\n",
    "        scores.append(f1)\n",
    "\n",
    "    return np.mean(scores, axis=0)\n",
    "\n",
    "def is_url(url):\n",
    "  try:\n",
    "    result = urlparse(url)\n",
    "    return all([result.scheme, result.netloc])\n",
    "  except ValueError:\n",
    "    return False"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\BENLEE~1\\AppData\\Local\\Temp/ipykernel_7212/876445206.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"svm\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mlabels_pred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "model = get_model(\"svm\")\n",
    "model.fit(X=features, y=labels)\n",
    "labels_pred = model.predict(features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\BENLEE~1\\AppData\\Local\\Temp/ipykernel_7212/178972305.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mf1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mf1_score\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels_pred\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maverage\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(labels, labels_pred, average=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\BENLEE~1\\AppData\\Local\\Temp/ipykernel_7212/667394252.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mf1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'f1' is not defined"
     ]
    }
   ],
   "source": [
    "f1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-384604ca-5f01-4fb1-a6de-8cf027ccfe59",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b8e7fefa",
    "execution_start": 1641236737861,
    "execution_millis": 663,
    "deepnote_cell_type": "code"
   },
   "source": [
    "x_text, labels_og = load_data()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: ../data/twitter_data_fixed.pkl\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "labels, uniques = pd.factorize(labels_og)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00005-7a994976-4250-473c-9365-7c598a214c5e",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3328cee7",
    "execution_start": 1641236894129,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": [
    "comments = pd.DataFrame({'comment': x_text, 'attack': labels})\n",
    "\n",
    "# decode to UTF-8\n",
    "comments['comment'] = comments['comment'].str.decode(\"utf-8\")\n",
    "\n",
    "# remove missing rows\n",
    "comments['comment'].dropna(inplace=True)\n",
    "\n",
    "# remove usernames\n",
    "comments['comment'] = comments['comment'].str.replace('(\\@\\w+.*?)',\"\")\n",
    "\n",
    "# lower case everything\n",
    "comments['comment'] = comments['comment'].str.lower()\n",
    "\n",
    "# remove URLs\n",
    "comments['comment'] = [' '.join(y for y in x.split() if not is_url(y)) for x in comments['comment']]\n",
    "\n",
    "# remove stop words\n",
    "comments['comment'] = comments['comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))\n",
    "\n",
    "# tokenize\n",
    "tt = TweetTokenizer()\n",
    "comments['comment'] = [tt.tokenize(entry) for entry in comments['comment']]\n",
    "\n",
    "# remove punctuation\n",
    "comments['comment'] = [list(filter(lambda x: x not in string.punctuation, sentence)) for sentence in comments['comment']]\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BENLEE~1\\AppData\\Local\\Temp/ipykernel_7212/2086462760.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  comments['comment'] = comments['comment'].str.replace('(\\@\\w+.*?)',\"\")\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "comment    object\nattack      int64\ndtype: object"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 comment  attack\n",
      "0      [rt, another, bloody, instant, restaurant, wee...       0\n",
      "1      [video, peshmerga, decimating, isis, far, inte...       0\n",
      "2      [oh, really, instant, restaurants, that's, sho...       0\n",
      "3      [rt, good, weeks, #isis, new, front, opened, #...       0\n",
      "4      [rt, don, â€™, t, need, femisnsn, men, carry, he...       0\n",
      "...                                                  ...     ...\n",
      "16085  [rt, i, want, equal, rights, still, want, seat...       2\n",
      "16086  [rt, go, ahead, call, sexist, scandalous, wome...       2\n",
      "16087  [epic, always, kept, plugged, in, plugged, use...       0\n",
      "16088  [think, daesh, planning, second, battle, trenc...       0\n",
      "16089  [rt, skin, green, colors, suit, wear, ripped, ...       0\n",
      "\n",
      "[16090 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(comments)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "textdata = comments['comment']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "comments.to_csv(\"../data/twitter_data_preprocessed2.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00004-b96dcd04-6aa5-4de5-a6dd-b9b2260004a6",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b475262a",
    "execution_start": 1641237143793,
    "execution_millis": 1126364,
    "deepnote_output_heights": [
     null,
     607
    ],
    "deepnote_cell_type": "code"
   },
   "source": [
    "models = [\"lr\", \"svm\", \"rf\", \"nb\"]\n",
    "representations = [\"character n-grams\", \"word unigrams\"]\n",
    "n_folds = 10\n",
    "\n",
    "results = []\n",
    "for rep in representations:\n",
    "    if rep == \"word unigrams\":\n",
    "        transformer = CountVectorizer(analyzer=\"word\", stop_words=\"english\")\n",
    "    else:\n",
    "        transformer = CountVectorizer(analyzer = \"char\", ngram_range=(1,2))\n",
    "    count_textdata = transformer.fit_transform(textdata)\n",
    "    freq_transfomer = TfidfTransformer()\n",
    "    freq_textdata = freq_transfomer.fit_transform(count_textdata)\n",
    "    features = freq_textdata\n",
    "\n",
    "    scores = []\n",
    "    for modelname in models:\n",
    "        print(rep, modelname)\n",
    "        f1_scores = evaluate_model(modelname, features, labels)\n",
    "        scores.append(f1_scores)\n",
    "\n",
    "    results.append(pd.Series(scores, index=models))\n",
    "\n",
    "df = pd.concat(results, axis=0, keys=representations)"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\BENLEE~1\\AppData\\Local\\Temp/ipykernel_7212/3169733304.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m         \u001B[0mtransformer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCountVectorizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manalyzer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"char\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mngram_range\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m     \u001B[0mcount_textdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtransformer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtextdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m     \u001B[0mfreq_transfomer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTfidfTransformer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[0mfreq_textdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfreq_transfomer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcount_textdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001B[0m in \u001B[0;36mfit_transform\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   1328\u001B[0m                     \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1329\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1330\u001B[1;33m         \u001B[0mvocabulary\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_count_vocab\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mraw_documents\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfixed_vocabulary_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1331\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1332\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbinary\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001B[0m in \u001B[0;36m_count_vocab\u001B[1;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[0;32m   1199\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mdoc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mraw_documents\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1200\u001B[0m             \u001B[0mfeature_counter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1201\u001B[1;33m             \u001B[1;32mfor\u001B[0m \u001B[0mfeature\u001B[0m \u001B[1;32min\u001B[0m \u001B[0manalyze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1202\u001B[0m                 \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1203\u001B[0m                     \u001B[0mfeature_idx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvocabulary\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfeature\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001B[0m in \u001B[0;36m_analyze\u001B[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[0m\n\u001B[0;32m    111\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    112\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mpreprocessor\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 113\u001B[1;33m             \u001B[0mdoc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpreprocessor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    114\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mtokenizer\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    115\u001B[0m             \u001B[0mdoc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001B[0m in \u001B[0;36m_preprocess\u001B[1;34m(doc, accent_function, lower)\u001B[0m\n\u001B[0;32m     69\u001B[0m     \"\"\"\n\u001B[0;32m     70\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mlower\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 71\u001B[1;33m         \u001B[0mdoc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdoc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlower\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     72\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0maccent_function\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     73\u001B[0m         \u001B[0mdoc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0maccent_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00003-c6f56d38-9707-4a8c-ad3c-f8e0fb6a6583",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f804c160",
    "execution_start": 1641238270233,
    "execution_millis": 8,
    "deepnote_output_heights": [
     174.6875
    ],
    "deepnote_cell_type": "code"
   },
   "source": [
    "print(uniques)\n",
    "print(df)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['none' 'racism' 'sexism']\n",
      "character n-grams  lr     [0.8751335072171373, 0.685327978133294, 0.6180...\n",
      "                   svm    [0.8782619140294108, 0.6970001698013153, 0.642...\n",
      "                   rf     [0.8661223589261834, 0.534585081271872, 0.5346...\n",
      "                   nb     [0.8189128812320003, 0.02636348677077856, 0.13...\n",
      "word unigrams      lr     [0.8823246105420989, 0.6730464408470065, 0.646...\n",
      "                   svm    [0.8871198073094947, 0.721519829764152, 0.6932...\n",
      "                   rf     [0.8871867733014165, 0.7372396761164366, 0.665...\n",
      "                   nb     [0.8480896742923367, 0.40042297496296236, 0.39...\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00005-50edbf6d-397d-404f-8a67-c08f6337ed60",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c926ea5d",
    "execution_start": 1641238270293,
    "execution_millis": 441,
    "deepnote_cell_type": "code",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "df.name = \"F1_Twitter\"\n",
    "df.to_frame().to_csv(\"results_twitter.csv\", index_label=['processing', 'model', ['none', 'racism', 'sexism']])"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "1f258d41-bc49-4b9b-9d50-a7a491b90b36",
  "deepnote_execution_queue": []
 }
}