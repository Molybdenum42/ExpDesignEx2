LINK TO OVERLEAF
https://www.overleaf.com/5139488846tztmskpnwfzq

**Conclusion Issues**

- DL models outperform ML models
- Oversampling is better due to class imbalance - (marginal, not consistent (difference only in random embeddings))

- word embedding methods have no difference - (THRE IS A STAT DIFFERENCE)
- no difference between DL models (LSTM generally weaker) against oversampled data

- TL WHAT IS THE CLAIM?? -

**Data Issues**
- Twitter data very specifically sourced focusing on keywords which would attract racist and sexist comments (e.g. #notallmen, My Kitchen Rules, Islam Terrorism, etc.)


**Methodology Issues**
Python 2 vs Python 3 (shouldn't be an issue in concept)
TensorFlow vocabulary processor doesn't exist anymore
No train-test split?? Only 5-fold CV - (80/20)
OG People used a Branching CNN instead of a traditional CNN - have changed this as architecture not well-understood
No tuning on ML models
Oversampling done before t/t split

**Replicability Issues**
Table 1 - Vocab size, unknown
Table 2 - Mostly right, questionable methodology on determination of 'swear word'
Table 3 - Baseline ML model not similar at all (even when copying code)

Next Catchup - 29.01, 10am

TODO - Report (DG, IF, BL)
TODO - Add Evaluation on Proper ML (DG)
TODO - Copy paste Formspring results, send to Ilija (BL)
TODO - Copy paste Twitter/Wiki results
TODO - Transfer Learning - write some bullshit (BL)
TODO - Clean up Github

Report Structure
ABSTRACT (Do at end)
INTRODUCTION (Describe plan on paper, etc - focus on talking about PRIMAD)
 - focus on Repeating Validation of statements which are: 1, 2, 3, 4, 5, 6, 7
ISSUES FOUND IN ORIGINAL STUDY
OUR OWN SETUP (Out of necessity, primed Port and partial Recode)
RESULTS: Discuss each statement
CONCLUSION
FUTURE WORK
REFERENCE


