{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ben\n",
      "[nltk_data]     Lee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import preprocessing as tfkp\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import gensim.downloader as api\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from DL_models import lstm_keras, cnn_keras, blstm, blstm_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    filename = \"../data/formspring_data_fixed.pkl\"\n",
    "    print(\"Loading data from file: \" + filename)\n",
    "    data = pickle.load(open(filename, 'rb'))\n",
    "    x_text = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "    for i in range(len(data)):\n",
    "        text = \"\".join(l for l in data[i]['text'] if l not in string.punctuation)\n",
    "        x_text.append((data[i]['text']).encode('utf-8'))\n",
    "        labels.append(data[i]['label'])\n",
    "    return x_text,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def is_url(url):\n",
    "  try:\n",
    "    result = urlparse(url)\n",
    "    return all([result.scheme, result.netloc])\n",
    "  except ValueError:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# embedding stuff\n",
    "def map_weights(embed_dict, vocab, embed_size): # embed size is embedding dim\n",
    "    vocab_size = len(vocab) + 1\n",
    "    weights = np.zeros((vocab_size, embed_size))\n",
    "\n",
    "    n_missed = 0\n",
    "    words_missed = []\n",
    "    for k,v in vocab.items():\n",
    "        try:\n",
    "            weights[v] = embed_dict[k]  # weights[v] is an index, embed_dict[k] is the list of weights\n",
    "        except:\n",
    "            n_missed += 1\n",
    "            words_missed.append(k)\n",
    "    print(f\"{n_missed} embeddings missed of {vocab_size}\")\n",
    "    return weights, words_missed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: ../data/formspring_data_fixed.pkl\n"
     ]
    }
   ],
   "source": [
    "x_text, labels_og = load_data()\n",
    "labels, uniques = pd.factorize(labels_og)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Intitial Twitter-Specific Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "comments = pd.DataFrame({'comment': x_text, 'attack': labels})\n",
    "\n",
    "# decode to UTF-8\n",
    "comments['comment'] = comments['comment'].str.decode(\"utf-8\")\n",
    "\n",
    "# lower case everything\n",
    "comments['comment'] = comments['comment'].str.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "comments['comment'] = comments['comment'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n",
    "\n",
    "#Remove words containing numbers\n",
    "comments['comment'] = comments['comment'].apply(lambda x: re.sub('\\w*\\d\\w*', '', x))\n",
    "\n",
    "#Remove in sentece white spaces\n",
    "comments['comment'] = comments['comment'].apply(lambda x: re.sub(' +', ' ', x))\n",
    "\n",
    "#Remove whitespaces at begining and end of sentence\n",
    "comments['comment'] = comments['comment'].str.strip()\n",
    "\n",
    "# remove stop words\n",
    "comments['comment'] = comments['comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))\n",
    "\n",
    "# remove empty string rows\n",
    "comments = comments[comments['comment'] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.to_pickle('../data/formspring_final_preprocess.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>favorite song like many songs favorite</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>haha jk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey angel duh sexy really thanks haha</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meowww rawr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>makeup tips suck makeup lol sure like tell wht...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12768</th>\n",
       "      <td>youre party friend drove drunk wo give keys wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12769</th>\n",
       "      <td>awesome give compliment deserve thank awesome</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12770</th>\n",
       "      <td>yu play yurself time sometimes day</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12771</th>\n",
       "      <td>yukk beer disgusting drink im already drunk br...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12772</th>\n",
       "      <td>told u den would make less fun would make yuh ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12735 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  attack\n",
       "0                 favorite song like many songs favorite       0\n",
       "1                                                haha jk       0\n",
       "2                  hey angel duh sexy really thanks haha       0\n",
       "4                                            meowww rawr       0\n",
       "5      makeup tips suck makeup lol sure like tell wht...       0\n",
       "...                                                  ...     ...\n",
       "12768  youre party friend drove drunk wo give keys wo...       0\n",
       "12769      awesome give compliment deserve thank awesome       0\n",
       "12770                 yu play yurself time sometimes day       0\n",
       "12771  yukk beer disgusting drink im already drunk br...       0\n",
       "12772  told u den would make less fun would make yuh ...       0\n",
       "\n",
       "[12735 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle('../data/formspring_final_preprocess.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train-Test Split and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pretoken, X_midway_pretoken, y_train, y_midway = train_test_split(comments['comment'], comments['attack'], random_state = 42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dim = 25\n",
    "n_classes = len(np.unique(y_train.values))\n",
    "\n",
    "tokenizer = tfkp.text.Tokenizer(oov_token=\"<UNK>\", filters='!\"$%#&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',)\n",
    "tokenizer.fit_on_texts(X_train_pretoken)\n",
    "\n",
    "convert = lambda x: tfkp.sequence.pad_sequences(tokenizer.texts_to_sequences(x),\n",
    "                                                    maxlen=dim,\n",
    "                                                    padding='post', truncating='post')\n",
    "\n",
    "X_train = convert(X_train_pretoken)\n",
    "X_midway = convert(X_midway_pretoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_midway, y_midway, random_state = 42, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(random_state=42)\n",
    "X_train_over, y_train_over = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Reshaping for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_train_onehot = to_categorical(y_train, n_classes)\n",
    "y_val_onehot = to_categorical(y_val, n_classes)\n",
    "y_test_onehot = to_categorical(y_test, n_classes)\n",
    "y_train_over_onehot = to_categorical(y_train_over, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Glove Embedding Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2629 embeddings missed of 15475\n"
     ]
    }
   ],
   "source": [
    "glove_dict = api.load(\"glove-twitter-200\")\n",
    "glove_weights, glove_words_missed = map_weights(glove_dict, tokenizer.word_index, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def replace(string, char):\n",
    "#     pattern = char + '{3,}'\n",
    "#     string = re.sub(pattern, char, string)\n",
    "#     return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace('hiii', collections.Counter('hiii').most_common(1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import collections\n",
    "# print(collections.Counter(glove_words_missed[1]).most_common(1)[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Word2vec Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4852 embeddings missed of 15475\n"
     ]
    }
   ],
   "source": [
    "word_dict = api.load(\"word2vec-google-news-300\")\n",
    "word2vec_weights, word2vec_words_missed = map_weights(word_dict, tokenizer.word_index, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 9580, 1: 608})\n",
      "Counter({0: 9580, 1: 9580})\n"
     ]
    }
   ],
   "source": [
    "# check number of oversampling\n",
    "from collections import Counter\n",
    "print(Counter(y_train))\n",
    "print(Counter(y_train_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Write Embedders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "random_embedder = Embedding(vocab_size, 200, input_length=dim, trainable=True)\n",
    "glove_embedding = Embedding(vocab_size, 200, input_length=dim, embeddings_initializer=initializers.Constant(glove_weights),\n",
    "                            trainable=False)\n",
    "word2vec_embedding = Embedding(vocab_size, 300, input_length=dim, embeddings_initializer=initializers.Constant(word2vec_weights),\n",
    "                              trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load DL Models and Run Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lstm_random = lstm_keras(dim,n_classes,random_embedder)\n",
    "lstm_random_over = lstm_keras(dim,n_classes,random_embedder)\n",
    "lstm_glove = lstm_keras(dim,n_classes,glove_embedding)\n",
    "lstm_glove_over = lstm_keras(dim,n_classes,glove_embedding)\n",
    "lstm_word2vec = lstm_keras(dim,n_classes,word2vec_embedding)\n",
    "lstm_word2vec_over = lstm_keras(dim,n_classes,word2vec_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "blstm_random = blstm(dim,n_classes,random_embedder)\n",
    "blstm_random_over = blstm(dim,n_classes,random_embedder)\n",
    "blstm_glove = blstm(dim,n_classes,glove_embedding)\n",
    "blstm_glove_over = blstm(dim,n_classes,glove_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "blstm_att_random = blstm_att(dim,n_classes,random_embedder)\n",
    "blstm_att_random_over = blstm_att(dim,n_classes,random_embedder)\n",
    "blstm_att_glove = blstm_att(dim,n_classes,glove_embedding)\n",
    "blstm_att_glove_over = blstm_att(dim,n_classes,glove_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cnn_random = cnn_keras(dim,n_classes,random_embedder)\n",
    "cnn_random_over = cnn_keras(dim,n_classes,random_embedder)\n",
    "cnn_glove = cnn_keras(dim,n_classes,glove_embedding)\n",
    "cnn_glove_over = cnn_keras(dim,n_classes,glove_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cnn_random.fit(X_train, y_train_onehot, epochs=3, validation_data=(X_val, y_val_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lstm_random.fit(X_train,  y_train_onehot, epochs=3, validation_data=(X_val, y_val_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lstm_glove.fit(X_train,  y_train_onehot, epochs=3, validation_data=(X_val, y_val_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lstm_word2vec.fit(X_train,  y_train_onehot, epochs=3, validation_data=(X_val, y_val_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "blstm_random_over.fit(X_train_over,  y_train_over_onehot, epochs=3, validation_data=(X_val, y_val_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "blstm_att_random_over.fit(X_train_over,  y_train_over_onehot, epochs=3, validation_data=(X_val, y_val_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preds_probs = blstm_att_random_over.predict(X_test)\n",
    "preds = [np.argmax(p) for p in preds_probs]\n",
    "\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(confusion_matrix(y_test, preds, normalize='true'), fmt='.2%', annot=True ,linewidths=.5)#,cmap='YlOrRd', annot_kws={\"fontsize\":10})#, yticklabels=target_names, xticklabels=target_names)\n",
    "plt.xticks(rotation=45, ha='right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}