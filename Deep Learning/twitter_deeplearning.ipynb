{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from urllib.parse import urlparse\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import preprocessing as tfkp\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ben\n",
      "[nltk_data]     Lee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def load_data():\n",
    "    filename = \"../data/twitter_data_fixed.pkl\"\n",
    "    print(\"Loading data from file: \" + filename)\n",
    "    data = pickle.load(open(filename, 'rb'))\n",
    "    x_text = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "    for i in range(len(data)):\n",
    "        text = \"\".join(l for l in data[i]['text'] if l not in string.punctuation)\n",
    "        x_text.append((data[i]['text']).encode('utf-8'))\n",
    "        labels.append(data[i]['label'])\n",
    "    return x_text,labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def is_url(url):\n",
    "  try:\n",
    "    result = urlparse(url)\n",
    "    return all([result.scheme, result.netloc])\n",
    "  except ValueError:\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "# embedding stuff\n",
    "\n",
    "def map_weights(embed_dict, vocab, embed_size): # embed size is embedding dim\n",
    "    vocab_size = len(vocab) + 1\n",
    "    weights = np.zeros((vocab_size, embed_size))\n",
    "\n",
    "    n_missed = 0\n",
    "    words_missed = []\n",
    "    for k,v in vocab.items():\n",
    "        try:\n",
    "            weights[v] = embed_dict[k]  # weights[v] is an index, embed_dict[k] is the list of weights\n",
    "        except:\n",
    "            n_missed += 1\n",
    "            words_missed.append(k)\n",
    "    print(f\"{n_missed} embeddings missed of {vocab_size}\")\n",
    "    return weights, words_missed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: ../data/twitter_data_fixed.pkl\n"
     ]
    }
   ],
   "source": [
    "x_text, labels_og = load_data()\n",
    "labels, uniques = pd.factorize(labels_og)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Intitial Twitter-Specific Pre-processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "comments = pd.DataFrame({'comment': x_text, 'attack': labels})\n",
    "\n",
    "# decode to UTF-8\n",
    "comments['comment'] = comments['comment'].str.decode(\"utf-8\")\n",
    "\n",
    "# remove missing rows\n",
    "comments['comment'].dropna(inplace=True)\n",
    "\n",
    "# remove usernames\n",
    "comments['comment'] = comments['comment'].str.replace('(\\@\\w+.*?)',\"\", regex=True)\n",
    "\n",
    "# lower case everything\n",
    "comments['comment'] = comments['comment'].str.lower()\n",
    "\n",
    "# remove URLs\n",
    "comments['comment'] = [' '.join(y for y in x.split() if not is_url(y)) for x in comments['comment']]\n",
    "\n",
    "# remove stop words\n",
    "comments['comment'] = comments['comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))\n",
    "\n",
    "# traditionally, would also lemmatize but this was not done in the main data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             comment  attack\n0  rt : another bloody instant restaurant week ? ...       0\n1   video peshmerga decimating isis far interesting.       0\n2  oh really ? instant restaurants ? that's shock...       0\n3  rt : good weeks #isis. new front opened #sinja...       0\n4  rt : don’t need femisnsn men carry heavy thing...       0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment</th>\n      <th>attack</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>rt : another bloody instant restaurant week ? ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>video peshmerga decimating isis far interesting.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>oh really ? instant restaurants ? that's shock...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>rt : good weeks #isis. new front opened #sinja...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>rt : don’t need femisnsn men carry heavy thing...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head(n=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train-Test Split and Tokenization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "X_train_pretoken, X_midway_pretoken, y_train, y_midway = train_test_split(comments['comment'], comments['attack'], random_state = 42, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "dim = 25\n",
    "n_classes = len(np.unique(y_train.values))\n",
    "\n",
    "tokenizer = tfkp.text.Tokenizer(oov_token=\"<UNK>\", filters='!\"$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',)\n",
    "tokenizer.fit_on_texts(X_train_pretoken)\n",
    "\n",
    "convert = lambda x: tfkp.sequence.pad_sequences(tokenizer.texts_to_sequences(x),\n",
    "                                                    maxlen=dim,\n",
    "                                                    padding='post', truncating='post')\n",
    "\n",
    "X_train = convert(X_train_pretoken)\n",
    "X_midway = convert(X_midway_pretoken)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_midway, y_midway, random_state = 42, test_size=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Oversampling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(random_state=42)\n",
    "X_train_over, y_train_over = oversample.fit_resample(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reshaping for input"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "y_train_onehot = to_categorical(y_train, n_classes)\n",
    "y_val_onehot = to_categorical(y_val, n_classes)\n",
    "y_test_onehot = to_categorical(y_test, n_classes)\n",
    "y_train_over_onehot = to_categorical(y_train_over, n_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Glove Embedding Weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "glove_dict = api.load(\"glove-twitter-200\")\n",
    "glove_weights, glove_words_missed = map_weights(glove_dict, tokenizer.word_index, 200)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: glove_embeddings/glove.twitter.27B.200d.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove_embeddings/glove.twitter.27B.200d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\BENLEE~1\\AppData\\Local\\Temp/ipykernel_1208/609920431.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0membed_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_embeddings\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'twitter'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mglove_weights\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mglove_words_missed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmap_weights\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membed_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mword_index\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Users\\BENLEE~1\\AppData\\Local\\Temp/ipykernel_1208/14336192.py\u001B[0m in \u001B[0;36mget_embeddings\u001B[1;34m(dim, dataset)\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Loading data from file: \"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mvector_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[0membed_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvector_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"r\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"utf8\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mfile\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mfile\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreadlines\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m             \u001B[0mrow\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mline\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstrip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'glove_embeddings/glove.twitter.27B.200d.txt'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word2vec Weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\BENLEE~1\\AppData\\Local\\Temp/ipykernel_1208/938741261.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m#space here for word2vec\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mgensim\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeyedvectors\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mword2vec\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mwordvectors\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mword2vec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mKeyedVectors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_word2vec_format\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'GoogleNews-vectors-negative300.bin'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbinary\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mword2vec_weights\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwrod2vec_words_missed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmap_weights\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwordvectors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mword_index\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m300\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ben lee\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001B[0m in \u001B[0;36mload_word2vec_format\u001B[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001B[0m\n\u001B[0;32m   1627\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1628\u001B[0m         \"\"\"\n\u001B[1;32m-> 1629\u001B[1;33m         return _load_word2vec_format(\n\u001B[0m\u001B[0;32m   1630\u001B[0m             \u001B[0mcls\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfvocab\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mfvocab\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbinary\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbinary\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mencoding\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0municode_errors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0municode_errors\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1631\u001B[0m             \u001B[0mlimit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdatatype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdatatype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mno_header\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mno_header\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ben lee\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001B[0m in \u001B[0;36m_load_word2vec_format\u001B[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001B[0m\n\u001B[0;32m   1953\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1954\u001B[0m     \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"loading projection weights from %s\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1955\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'rb'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mfin\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1956\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mno_header\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1957\u001B[0m             \u001B[1;31m# deduce both vocab_size & vector_size from 1st pass over file\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ben lee\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001B[0m in \u001B[0;36mopen\u001B[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001B[0m\n\u001B[0;32m    186\u001B[0m         \u001B[0mtransport_params\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    187\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 188\u001B[1;33m     fobj = _shortcut_open(\n\u001B[0m\u001B[0;32m    189\u001B[0m         \u001B[0muri\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    190\u001B[0m         \u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ben lee\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001B[0m in \u001B[0;36m_shortcut_open\u001B[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001B[0m\n\u001B[0;32m    359\u001B[0m         \u001B[0mopen_kwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'errors'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    360\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 361\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_builtin_open\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlocal_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbuffering\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbuffering\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mopen_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    362\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    363\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "word_dict = api.load(\"word2vec-google-news-300\")\n",
    "word2vec_weights, wrod2vec_words_missed = map_weights(word_dict, tokenizer.word_index, 300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Checks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12872, 25)\n",
      "(26508, 25)\n",
      "(1609, 25)\n",
      "(1609, 25)\n",
      "(12872,)\n",
      "(26508,)\n",
      "(1609,)\n",
      "(1609,)\n",
      "(15155, 200)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(X_train_over))\n",
    "print(np.shape(X_val))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(y_train_over))\n",
    "print(np.shape(y_val))\n",
    "print(np.shape(y_test))\n",
    "print(np.shape(glove_weights))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26508, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(y_train_over_onehot))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 8836, 2: 2503, 1: 1533})\n",
      "Counter({2: 8836, 0: 8836, 1: 8836})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(y_train))\n",
    "print(Counter(y_train_over))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Write Embedders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "random_embedder = Embedding(vocab_size, 200, input_length=dim, trainable=True)\n",
    "glove_embedding = Embedding(vocab_size, 200, embeddings_initializer=initializers.Constant(glove_weights),\n",
    "                            trainable=False)\n",
    "word2vec_embedding = Embedding(vocab_size, 300, embeddings_initializer=initializers.Constant(word2vec_weights),\n",
    "                              trainable=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load DL Models and Run Them"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "from DL_models import lstm_keras, cnn_keras, blstm, blstm_atten"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [],
   "source": [
    "lstm_random = lstm_keras(dim,n_classes,random_embedder)\n",
    "lstm_random_over = lstm_keras(dim,n_classes,random_embedder)\n",
    "lstm_glove = lstm_keras(dim,n_classes,glove_embedding)\n",
    "lstm_glove_over = lstm_keras(dim,n_classes,glove_embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "blstm_random = blstm(dim_,n_classes,random_embedder)\n",
    "blstm_random_over = blstm(dim,n_classes,random_embedder)\n",
    "blstm_glove = blstm(dim,n_classes,glove_embedding)\n",
    "blstm_glove_over = blstm(dim,n_classes,glove_embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\BENLEE~1\\AppData\\Local\\Temp/ipykernel_1208/2613454773.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m#BLSTM and CNN need work\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mblstm_atten_random\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mblstm_atten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membed_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mn_classes\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mrandom_embedder\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mblstm_atten_random_over\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mblstm_atten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membed_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mn_classes\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mrandom_embedder\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mblstm_atten_glove\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mblstm_atten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membed_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mn_classes\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mglove_embedding\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mG:\\My Drive\\Uni\\Experiment Design\\Ass2\\Deep Learning\\DL_models.py\u001B[0m in \u001B[0;36mblstm_atten\u001B[1;34m(embed_size, num_classes, embedder)\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ben lee\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    528\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    529\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 530\u001B[1;33m       \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    531\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    532\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprevious_value\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ben lee\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 67\u001B[1;33m       \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     68\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m       \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ben lee\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\sequential.py\u001B[0m in \u001B[0;36madd\u001B[1;34m(self, layer)\u001B[0m\n\u001B[0;32m    220\u001B[0m       \u001B[0moutput_tensor\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlayer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    221\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput_tensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 222\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    223\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0moutput_tensor\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    224\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuilt\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API."
     ]
    }
   ],
   "source": [
    "#BLSTM and CNN need work\n",
    "\n",
    "blstm_atten_random = blstm_atten(dim,n_classes,random_embedder)\n",
    "blstm_atten_random_over = blstm_atten(embed_size,n_classes,random_embedder)\n",
    "blstm_atten_glove = blstm_atten(embed_size,n_classes,glove_embedding)\n",
    "blstm_atten_glove_over = blstm_atten(embed_size,n_classes,glove_embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"max_pooling1d_26\" (type MaxPooling1D).\n\nNegative dimension size caused by subtracting 35 from 1 for '{{node max_pooling1d_26/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 35, 1, 1], padding=\"VALID\", strides=[1, 35, 1, 1]](max_pooling1d_26/ExpandDims)' with input shapes: [?,1,1,64].\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 1, 64), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\BENLEE~1\\AppData\\Local\\Temp/ipykernel_1208/4238516101.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mcnn_random\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcnn_keras\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membed_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mn_classes\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mrandom_embedder\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mcnn_random_over\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcnn_keras\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membed_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mn_classes\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mrandom_embedder\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mcnn_glove\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcnn_keras\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membed_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mn_classes\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mglove_embedding\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mcnn_glove_over\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcnn_keras\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membed_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mn_classes\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mglove_embedding\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mG:\\My Drive\\Uni\\Experiment Design\\Ass2\\Deep Learning\\DL_models.py\u001B[0m in \u001B[0;36mcnn_keras\u001B[1;34m(embed_size, num_classes, embedder)\u001B[0m\n\u001B[0;32m     37\u001B[0m                   \u001B[0moptimizer\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'adam'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     38\u001B[0m                   metrics=['accuracy'])\n\u001B[1;32m---> 39\u001B[1;33m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     40\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ben lee\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    528\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    529\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 530\u001B[1;33m       \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    531\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    532\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprevious_value\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ben lee\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 67\u001B[1;33m       \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     68\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m       \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ben lee\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\backend.py\u001B[0m in \u001B[0;36mpool2d\u001B[1;34m(x, pool_size, strides, padding, data_format, pool_mode)\u001B[0m\n\u001B[0;32m   5873\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5874\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mpool_mode\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'max'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 5875\u001B[1;33m     x = tf.compat.v1.nn.max_pool(\n\u001B[0m\u001B[0;32m   5876\u001B[0m         x, pool_size, strides, padding=padding, data_format=tf_data_format)\n\u001B[0;32m   5877\u001B[0m   \u001B[1;32melif\u001B[0m \u001B[0mpool_mode\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'avg'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Exception encountered when calling layer \"max_pooling1d_26\" (type MaxPooling1D).\n\nNegative dimension size caused by subtracting 35 from 1 for '{{node max_pooling1d_26/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 35, 1, 1], padding=\"VALID\", strides=[1, 35, 1, 1]](max_pooling1d_26/ExpandDims)' with input shapes: [?,1,1,64].\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 1, 64), dtype=float32)"
     ]
    }
   ],
   "source": [
    "cnn_random = cnn_keras(embed_size,n_classes,random_embedder)\n",
    "cnn_random_over = cnn_keras(embed_size,n_classes,random_embedder)\n",
    "cnn_glove = cnn_keras(embed_size,n_classes,glove_embedding)\n",
    "cnn_glove_over = cnn_keras(embed_size,n_classes,glove_embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_73\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_22 (Embedding)    (None, 25, 200)           3031000   \n",
      "                                                                 \n",
      " dropout_146 (Dropout)       (None, 25, 200)           0         \n",
      "                                                                 \n",
      " lstm_61 (LSTM)              (None, 25)                22600     \n",
      "                                                                 \n",
      " dropout_147 (Dropout)       (None, 25)                0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 3)                 78        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,053,678\n",
      "Trainable params: 3,053,678\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "403/403 [==============================] - 10s 21ms/step - loss: 0.7019 - accuracy: 0.7216 - val_loss: 0.5702 - val_accuracy: 0.7918\n",
      "Epoch 2/3\n",
      "403/403 [==============================] - 8s 20ms/step - loss: 0.4336 - accuracy: 0.8525 - val_loss: 0.4568 - val_accuracy: 0.8365\n",
      "Epoch 3/3\n",
      "403/403 [==============================] - 8s 20ms/step - loss: 0.2662 - accuracy: 0.9172 - val_loss: 0.4901 - val_accuracy: 0.8297\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x20d240e0e50>"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_random.summary()\n",
    "lstm_random.fit(X_train,  y_train_onehot, epochs=3, validation_data=(X_val, y_val_onehot))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_23 (Embedding)    (None, None, 200)         3031000   \n",
      "                                                                 \n",
      " dropout_150 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " lstm_63 (LSTM)              (None, 25)                22600     \n",
      "                                                                 \n",
      " dropout_151 (Dropout)       (None, 25)                0         \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 3)                 78        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,053,678\n",
      "Trainable params: 22,678\n",
      "Non-trainable params: 3,031,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "403/403 [==============================] - 4s 7ms/step - loss: 0.6508 - accuracy: 0.7509 - val_loss: 0.4919 - val_accuracy: 0.8117\n",
      "Epoch 2/3\n",
      "403/403 [==============================] - 3s 7ms/step - loss: 0.5081 - accuracy: 0.8097 - val_loss: 0.4635 - val_accuracy: 0.8316\n",
      "Epoch 3/3\n",
      "403/403 [==============================] - 3s 6ms/step - loss: 0.4804 - accuracy: 0.8174 - val_loss: 0.4519 - val_accuracy: 0.8229\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x20d31347070>"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_glove.summary()\n",
    "lstm_glove.fit(X_train,  y_train_onehot, epochs=3, validation_data=(X_val, y_val_onehot))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_20 (Embedding)    (None, 25, 25)            378875    \n",
      "                                                                 \n",
      " dropout_132 (Dropout)       (None, 25, 25)            0         \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirectio  (None, 50)               10200     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_133 (Dropout)       (None, 50)                0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 389,228\n",
      "Trainable params: 389,228\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "829/829 [==============================] - 10s 9ms/step - loss: 0.5020 - accuracy: 0.7846 - val_loss: 0.5347 - val_accuracy: 0.8086\n",
      "Epoch 2/3\n",
      "829/829 [==============================] - 7s 8ms/step - loss: 0.1915 - accuracy: 0.9364 - val_loss: 0.5850 - val_accuracy: 0.8204\n",
      "Epoch 3/3\n",
      "829/829 [==============================] - 7s 8ms/step - loss: 0.1209 - accuracy: 0.9613 - val_loss: 0.6508 - val_accuracy: 0.8266\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x20d24100520>"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blstm_random_over.summary()\n",
    "blstm_random_over.fit(X_train_over,  y_train_over_onehot, epochs=3, validation_data=(X_val, y_val_onehot))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}